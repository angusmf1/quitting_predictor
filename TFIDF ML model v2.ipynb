{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c2ae0e",
   "metadata": {},
   "source": [
    "The dataset underwent a series of steps to enhance its usability and eliminate noise. Below is an overview of the cleaning and preprocessing tasks performed with TFIDF process:\n",
    "\n",
    "### Data Filtering:\n",
    "\n",
    "Invalid or illogical data entries were removed, including instances with false experience (exp_check == 1), negative experience in any job_duration column, and total experience exceeding 900 months.\n",
    "Data records associated with job titles containing terms like 'intern', 'student', 'owner', etc., were also filtered out to focus on meaningful job roles.\n",
    "\n",
    "### Text Preprocessing:\n",
    "\n",
    "The 'Summary', 'Skills', and 'Job_Description_1' columns underwent comprehensive text preprocessing to standardize and enhance the quality of textual data.\n",
    "Text was converted to lowercase, special characters and numbers were removed, and tokenization was performed to break down the text into individual words.\n",
    "Stop words (commonly used words with little semantic value) were eliminated, and lemmatization was applied to reduce words to their base form.\n",
    "\n",
    "### TF-IDF Vectorization:\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) vectorization was employed to convert the preprocessed text data into numerical feature vectors suitable for machine learning algorithms.\n",
    "Separate TF-IDF vectorizers were created for 'Job_Description_1', 'Summary', and 'Skills' columns.\n",
    "The resulting features were combined into a unified dataset, ensuring comprehensive coverage of relevant information.\n",
    "\n",
    "### Feature Selection and Integration:\n",
    "\n",
    "Features derived from TF-IDF vectorization were selected based on a specified maximum number of features.\n",
    "Additional features, such as 'Word_Count', 'summary_is_null', and 'skills_is_null', were incorporated into the final dataset to provide contextual information for predictive modeling.\n",
    "\n",
    "### Saving Processed Data:\n",
    "\n",
    "The processed dataset, containing the selected features and relevant additional information, was saved as a CSV file ('text_data_t0.csv') for ease of access and future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e2b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import scipy.sparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1527402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jainish\\AppData\\Local\\Temp\\ipykernel_4352\\797823496.py:1: DtypeWarning: Columns (61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('ready_train_w_regions.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ready_train_w_regions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b14aa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['profile_url', 'First Name', 'Middle Name', 'Surname', 'Linkedin_url',\n",
      "       'connections', 'Summary', 'Skills', 'education level', 'title_1',\n",
      "       'company_1', 'time_duration_1', 'Job_Description_1', 'title_2',\n",
      "       'company_2', 'time_duration_2', 'Job_Description_2', 'title_3',\n",
      "       'company_3', 'time_duration_3', 'Job_Description_3', 'title_4',\n",
      "       'company_4', 'time_duration_4', 'Job_Description_4', 'title_5',\n",
      "       'company_5', 'time_duration_5', 'Job_Description_5', 'title_6',\n",
      "       'company_6', 'time_duration_6', 'Job_Description_6', 'title_7',\n",
      "       'company_7', 'time_duration_7', 'Job_Description_7', 'title_8',\n",
      "       'company_8', 'time_duration_8', 'Job_Description_8', 'title_9',\n",
      "       'company_9', 'time_duration_9', 'Job_Description_9', 'title_10',\n",
      "       'company_10', 'time_duration_10', 'Job_Description_10', 'school_name_1',\n",
      "       'ed_time_duration_1', 'degree_name_1', 'education_fos_1',\n",
      "       'school_name_2', 'ed_time_duration_2', 'degree_name_2',\n",
      "       'education_fos_2', 'school_name_3', 'ed_time_duration_3',\n",
      "       'degree_name_3', 'education_fos_3', 'Country', 'State', 'City',\n",
      "       'job 1 location', 'job 2 location', 'job 3 location', 'job 4 location',\n",
      "       'job 5 location', 'job 6 location', 'job 7 location', 'job 8 location',\n",
      "       'job 9 location', 'job 10 location', 'school_name_4',\n",
      "       'ed_time_duration_4', 'degree_name_4', 'edu 4 activities',\n",
      "       'education_fos_4', 'school_name_5', 'ed_time_duration_5',\n",
      "       'degree_name_5', 'edu 5 activities', 'education_fos_5', 'school_name_6',\n",
      "       'ed_time_duration_6', 'degree_name_6', 'edu 6 activities',\n",
      "       'education_fos_6', 'level', 'total_exp', 'exp_check', 'Quitter',\n",
      "       'custom_url', 'Region'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9047fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('total_exp', axis=1)\n",
    "time_duration_cols = ['time_duration_10', 'time_duration_9', 'time_duration_8',\n",
    "                      'time_duration_6', 'time_duration_5', 'time_duration_4',\n",
    "                      'time_duration_3', 'time_duration_2', 'time_duration_1']\n",
    "\n",
    "df = df[df['exp_check'] == 0]\n",
    "df['has_negative_value'] = df[time_duration_cols].apply(lambda row: 1 if any(val < 0 for val in row) else 0, axis=1)\n",
    "df = df[df['has_negative_value'] == 0]\n",
    "df['average_duration'] = df[time_duration_cols].mean(axis=1)\n",
    "df['total_exp'] = df[time_duration_cols].sum(axis=1, skipna=True)\n",
    "df = df[df['total_exp'] < 900]\n",
    "df['Word_Count'] = df['Job_Description_1'].apply(lambda text: len(str(text).split()) if pd.notnull(text) else 0)\n",
    "median_word_count = df['Word_Count'].median()\n",
    "df['Word_Count'] = df['Word_Count'].replace(0, median_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3265e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary_is_null'] = df['Job_Description_1'].isnull().astype(int)\n",
    "df['skills_is_null'] = df['Skills'].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde24f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null counts\n",
      "profile_url               0\n",
      "Job_Description_1    124189\n",
      "Summary               59201\n",
      "Skills                88626\n",
      "Word_Count                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_counts_target = df[['profile_url','Job_Description_1', 'Summary', 'Skills', 'Word_Count']].isnull().sum()\n",
    "print(\"Null counts\")\n",
    "print(null_counts_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1af0e0",
   "metadata": {},
   "source": [
    "### Data Pre-processiong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "311da979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Jainish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jainish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jainish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "# Download stopwords and WordNet data from NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):  # Check if the text is null\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))  # Use set(stopwords) instead of `stopwords`\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    processed_text = \" \".join(tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5279271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_skills(skills):\n",
    "    if pd.isnull(skills):  # Check if the skills are null\n",
    "        return ''\n",
    "\n",
    "    skills = skills.lower()  # Convert skills to lowercase\n",
    "\n",
    "    # Split the skills by commas\n",
    "    skill_list = skills.split(',')\n",
    "\n",
    "    # Process each individual skill\n",
    "    processed_skills = []\n",
    "    for skill in skill_list:\n",
    "        # Remove leading/trailing whitespace\n",
    "        skill = skill.strip()\n",
    "\n",
    "        # Replace space with underscore\n",
    "        skill = skill.replace(' ', '_')\n",
    "\n",
    "        # Remove special characters and numbers\n",
    "        skill = re.sub(r\"[^a-zA-Z]\", \" \", skill)\n",
    "\n",
    "        # Tokenize the skill\n",
    "        tokens = word_tokenize(skill)\n",
    "\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        # Lemmatize the tokens\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        # Join the tokens back into a single string\n",
    "        processed_skill = \"_\".join(tokens)\n",
    "        processed_skills.append(processed_skill)\n",
    "\n",
    "    # Join the processed skills back into a single string\n",
    "    processed_text = \", \".join(processed_skills)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c54562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jainish\\AppData\\Local\\Temp\\ipykernel_4352\\232069428.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['Job_Description_1'] = df_selected['Job_Description_1'].apply(preprocess_text)\n",
      "C:\\Users\\Jainish\\AppData\\Local\\Temp\\ipykernel_4352\\232069428.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['Summary'] = df_selected['Summary'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "df_selected = df[['profile_url','Job_Description_1', 'Summary', 'Skills', 'Word_Count', 'summary_is_null', 'skills_is_null', 'Quitter']]\n",
    "\n",
    "df_selected['Job_Description_1'] = df_selected['Job_Description_1'].apply(preprocess_text)\n",
    "df_selected['Summary'] = df_selected['Summary'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef5e6ea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jainish\\AppData\\Local\\Temp\\ipykernel_4352\\3138566998.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['Skills'] = df_selected['Skills'].apply(preprocess_skills)\n"
     ]
    }
   ],
   "source": [
    "df_selected['Skills'] = df_selected['Skills'].apply(preprocess_skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f9005f",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cc47855",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = df_selected[['profile_url','Job_Description_1', 'Summary', 'Skills', 'Word_Count', 'summary_is_null', 'skills_is_null']]\n",
    "y = df_selected['Quitter']\n",
    "\n",
    "# Create TF-IDF vectorizer for Job_Description_1\n",
    "tfidf_job_desc = TfidfVectorizer(max_features=10, use_idf=False)\n",
    "X_job_desc_tfidf = tfidf_job_desc.fit_transform(X['Job_Description_1'])\n",
    "\n",
    "# Get the feature names for Job_Description_1 and modify them\n",
    "job_desc_feature_names = ['job_desc_' + name for name in tfidf_job_desc.get_feature_names_out()]\n",
    "\n",
    "# Create TF-IDF vectorizer for Summary\n",
    "tfidf_summary = TfidfVectorizer(max_features=10, use_idf=False)\n",
    "X_summary_tfidf = tfidf_summary.fit_transform(X['Summary'])\n",
    "\n",
    "# Get the feature names for Summary and modify them\n",
    "summary_feature_names = ['summary_' + name for name in tfidf_summary.get_feature_names_out()]\n",
    "\n",
    "# Create TF-IDF vectorizer for Skills\n",
    "tfidf_skills = TfidfVectorizer(max_features=10, use_idf=False)\n",
    "X_skills_tfidf = tfidf_skills.fit_transform(X['Skills'])\n",
    "skills_feature_names = ['skills_' + name for name in tfidf_skills.get_feature_names_out()]\n",
    "\n",
    "# Combine the TF-IDF features\n",
    "X_combined = pd.DataFrame.sparse.from_spmatrix(scipy.sparse.hstack([X_job_desc_tfidf, X_summary_tfidf, X_skills_tfidf]), columns=job_desc_feature_names + summary_feature_names + skills_feature_names)\n",
    "\n",
    "# Create a DataFrame with the selected features and Word_Count\n",
    "X_selected_df = X_combined.copy()\n",
    "X_selected_df['Word_Count'] = X['Word_Count']\n",
    "X_selected_df['summary_is_null'] = X['summary_is_null']\n",
    "X_selected_df['skills_is_null'] = X['skills_is_null']\n",
    "X_selected_df['profile_url'] = X['profile_url']\n",
    "\n",
    "X_selected_df.to_csv('text_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "372044ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['job_desc_application',\n",
       " 'job_desc_data',\n",
       " 'job_desc_design',\n",
       " 'job_desc_development',\n",
       " 'job_desc_project',\n",
       " 'job_desc_service',\n",
       " 'job_desc_software',\n",
       " 'job_desc_system',\n",
       " 'job_desc_team',\n",
       " 'job_desc_using',\n",
       " 'summary_application',\n",
       " 'summary_data',\n",
       " 'summary_development',\n",
       " 'summary_experience',\n",
       " 'summary_software',\n",
       " 'summary_system',\n",
       " 'summary_team',\n",
       " 'summary_technology',\n",
       " 'summary_web',\n",
       " 'summary_year',\n",
       " 'skills_agile_methodology',\n",
       " 'skills_cs',\n",
       " 'skills_html',\n",
       " 'skills_java',\n",
       " 'skills_javascript',\n",
       " 'skills_linux',\n",
       " 'skills_mysql',\n",
       " 'skills_python',\n",
       " 'skills_software_development',\n",
       " 'skills_sql',\n",
       " 'Word_Count',\n",
       " 'summary_is_null',\n",
       " 'skills_is_null',\n",
       " 'profile_url']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ceed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
